"""========================================================================================================
#
# ==============================================
# DatasetPreparer is a Python class designed for preprocessing image datasets.
# Its main capabilities include:
# 1. Scanning a specified dataset directory to find image files with given extensions (e.g., .jpg, .png),
#    and checking whether corresponding annotation files (e.g., .txt) exist.
# 2. Randomly splitting valid image-annotation pairs into training, validation, and test sets
#    based on user-defined ratios.
# 3. Writing the results into three files: train.txt, val.txt, and test.txt,
#    each listing the full paths of images in the corresponding set.
#
# Use Case:
# This tool is especially helpful for preparing datasets for object detection models such as YOLO,
# where automatic dataset organization is essential.
# ==============================================
# 
# # ==============================================
# DatasetPreparer æ˜¯ä¸€å€‹ç”¨æ–¼å½±åƒè³‡æ–™é›†å‰è™•ç†çš„ Python é¡åˆ¥ã€‚
# å®ƒçš„ä¸»è¦åŠŸèƒ½åŒ…æ‹¬ï¼š
# 1. æƒææŒ‡å®šè³‡æ–™å¤¾ä¸‹æ‰€æœ‰ç¬¦åˆå½±åƒå‰¯æª”åï¼ˆå¦‚ .jpg, .pngï¼‰çš„æª”æ¡ˆï¼Œ
#    ä¸¦ç¢ºèªæ˜¯å¦æœ‰å°æ‡‰çš„æ¨™è¨»æª”æ¡ˆï¼ˆå¦‚ .txtï¼‰ã€‚
# 2. å°‡ç¬¦åˆæ¢ä»¶çš„å½±åƒ-æ¨™è¨»å°ï¼Œä¾æ“šè¨“ç·´ã€é©—è­‰ã€æ¸¬è©¦çš„æ¯”ä¾‹ï¼ˆå¯è‡ªè¨‚ï¼‰é€²è¡Œéš¨æ©Ÿåˆ†å‰²ã€‚
# 3. å°‡åˆ†å‰²çµæœåˆ†åˆ¥å¯«å…¥ train.txtã€val.txtã€test.txt æª”æ¡ˆä¸­ï¼Œæ¯è¡Œç‚ºå½±åƒæª”æ¡ˆçš„å®Œæ•´è·¯å¾‘ã€‚
#
# ä½¿ç”¨æƒ…å¢ƒï¼š
# é©ç”¨æ–¼å¦‚ YOLO ç­‰ç‰©ä»¶åµæ¸¬æ¨¡å‹è¨“ç·´å‰çš„è³‡æ–™æº–å‚™éšæ®µï¼Œèƒ½å¤ è‡ªå‹•æƒæè³‡æ–™ä¸¦åˆ†å‰²æˆè¨“ç·´é›†ã€é©—è­‰é›†èˆ‡æ¸¬è©¦é›†ã€‚
# ==============================================
#
========================================================================================================"""

import os
import random
from typing import List, Tuple

class DatasetPreparer:
    def __init__(
        self,
        dataset_root: str,
        output_dir: str,
        train_ratio: float = 0.8,
        val_ratio: float = 0.15,
        test_ratio: float = 0.05,
        image_extensions: Tuple[str, ...] = ('.jpg', '.jpeg', '.png'),
        annotation_extension: str = '.txt',
        seed: int = 42
    ):
        self.dataset_root = dataset_root
        self.output_dir = output_dir
        self.train_ratio = train_ratio
        self.val_ratio = val_ratio
        self.test_ratio = test_ratio
        self.image_extensions = image_extensions
        self.annotation_extension = annotation_extension
        self.seed = seed
        self.image_label_pairs: List[str] = []

        self._validate_ratios()
        random.seed(self.seed)

    def _validate_ratios(self):
        total = self.train_ratio + self.val_ratio + self.test_ratio
        if not abs(total - 1.0) < 1e-6:
            raise ValueError("Train, validation, and test ratios must sum to 1.0")

    def collect_valid_pairs(self):
        print(f"Scanning '{self.dataset_root}' for image-label pairs...")
        for root, _, files in os.walk(self.dataset_root):
            for file in files:
                if file.lower().endswith(self.image_extensions):
                    image_path = os.path.join(root, file)
                    base_name = os.path.splitext(file)[0]
                    annotation_path = os.path.join(root, base_name + self.annotation_extension)
                    if os.path.exists(annotation_path):
                        self.image_label_pairs.append(image_path)
                    else:
                        print(f"Warning: Missing annotation for image '{image_path}'")
        print(f"Total valid image-label pairs found: {len(self.image_label_pairs)}")

    def split_dataset(self):
        print("Splitting dataset into train/val/test...")
        random.shuffle(self.image_label_pairs)
        total = len(self.image_label_pairs)
        train_end = int(total * self.train_ratio)
        val_end = train_end + int(total * self.val_ratio)

        train_set = self.image_label_pairs[:train_end]
        val_set = self.image_label_pairs[train_end:val_end]
        test_set = self.image_label_pairs[val_end:]

        print(f"Train set size: {len(train_set)}")
        print(f"Validation set size: {len(val_set)}")
        print(f"Test set size: {len(test_set)}")

        return train_set, val_set, test_set

    def write_split_files(self, train_set: List[str], val_set: List[str], test_set: List[str]):
        os.makedirs(self.output_dir, exist_ok=True)
        self._write_file('train.txt', train_set)
        self._write_file('val.txt', val_set)
        self._write_file('test.txt', test_set)
        print(f"Split files written to '{self.output_dir}'")

    def _write_file(self, filename: str, dataset: List[str]):
        path = os.path.join(self.output_dir, filename)
        with open(path, 'w') as f:
            for img_path in dataset:
                f.write(f"{img_path}\n")
        print(f"{filename}: {len(dataset)} entries")

    def run(self):
        self.collect_valid_pairs()
        train_set, val_set, test_set = self.split_dataset()
        self.write_split_files(train_set, val_set, test_set)

# =======================
# ğŸ› ï¸ Configuration Section
# =======================
if __name__ == "__main__":
    preparer = DatasetPreparer(
        dataset_root="datasets/human_dataset",         # Root folder containing subfolders of data
        output_dir="./output",            # Output folder for train.txt, val.txt, test.txt
        train_ratio=0.8,
        val_ratio=0.2,
        test_ratio=0.0,
        image_extensions=('.jpg', '.jpeg', '.png'),
        annotation_extension='.txt',
        seed=123
    )
    preparer.run()
